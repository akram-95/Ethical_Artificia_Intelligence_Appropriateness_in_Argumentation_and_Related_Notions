{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/akramchorfi/appropriateness-in-argumentation-and-related-notio?scriptVersionId=208037692\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import torch\nfrom datasets import load_dataset\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\nfrom torch.nn.utils.rnn import pad_sequence\nfrom transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, BertModel\n\n# Load the appropriateness dataset\ndataset = load_dataset('timonziegenbein/appropriateness-corpus')\n# Load a pre-trained BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Define all the labels (14 labels in total, including higher categories and subcategories)\nlabel_columns = [\n    'Inappropriateness', 'Toxic Emotions', 'Excessive Intensity', 'Emotional Deception',\n    'Missing Commitment', 'Missing Seriousness', 'Missing Openness',\n    'Missing Intelligibility', 'Unclear Meaning', 'Missing Relevance', 'Confusing Reasoning',\n    'Detrimental Orthography', 'Reason Unclassified', 'Other Reasons'\n]\n\n\n# Tokenization function to prepare the data for the BERT model\ndef tokenize_function(examples):\n    return tokenizer(examples['post_text'], padding=\"max_length\", truncation=True, max_length=128)\n\n\n# Tokenization function to prepare the data for the BERT model\ndef tokenize_and_encode_labels(examples):\n    # Tokenize with padding and truncation enabled\n    encoding = tokenizer(\n        examples['post_text'],\n        padding='max_length',  # Automatically pads the sequences to the same length\n        truncation=True,  # Automatically truncates longer sequences\n        return_tensors='pt'  # Return PyTorch tensors\n    )\n\n    # Proceed to handle labels\n    # Prepare the labels\n    labels = []\n    for i in range(len(examples[label_columns[0]])):  # Assuming all labels have the same length\n        example_labels = [examples[label][i] for label in label_columns]\n        labels.append(torch.tensor(example_labels, dtype=torch.float))\n\n    # Pad the labels if they are of variable length\n    labels_padded = pad_sequence(labels, batch_first=True, padding_value=0.0)\n\n    # Add labels tensor to the encoding\n    encoding['labels'] = labels_padded\n\n    return encoding\n\n\n# Apply the tokenization function to the dataset\ntokenized_datasets = dataset.map(tokenize_and_encode_labels, batched=True)\n\n# Set the columns for PyTorch compatibility\ntokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', \"labels\"])\n\n# Load a pre-trained BERT model for multi-label classification\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_columns))\n\n\n# Define the custom compute_metrics function for multi-label classification\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = (logits > 0).astype(int)  # Thresholding logits to get binary predictions\n    accuracy = accuracy_score(labels, predictions)\n    f1 = f1_score(labels, predictions, average=\"weighted\")\n    precision = precision_score(labels, predictions, average=\"weighted\")\n    recall = recall_score(labels, predictions, average=\"weighted\")\n    return {\n        \"accuracy\": accuracy,\n        \"f1\": f1,\n        \"precision\": precision,\n        \"recall\": recall,\n    }\n\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    save_strategy=\"epoch\"\n)\n\n# Trainer for fine-tuning BERT\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets['train'],\n    eval_dataset=tokenized_datasets['test'],\n    compute_metrics=compute_metrics\n)\n\n# Train the model\ntrainer.train()\n\n# Save the fine-tuned model\nmodel.save_pretrained('./appropriateness-classifier')\ntokenizer.save_pretrained('./appropriateness-classifier')","metadata":{"_uuid":"209e7a40-aaab-47b3-b74c-b119a84c2be5","_cell_guid":"d25045e3-dd9b-4a92-a97c-ca8c86c4ca2c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-17T23:13:32.824794Z","iopub.execute_input":"2024-11-17T23:13:32.825102Z","iopub.status.idle":"2024-11-17T23:19:00.301061Z","shell.execute_reply.started":"2024-11-17T23:13:32.825056Z","shell.execute_reply":"2024-11-17T23:19:00.300117Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/4.41k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6b55c70da5640259669528a421ca87a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train.csv:   0%|          | 0.00/850k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26c69683b67849f697ff057c18547126"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"valid.csv:   0%|          | 0.00/126k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6067bcad623b4c0dafd80ddc80c82ebc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test.csv:   0%|          | 0.00/243k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdd9f37bd0f14484a980a10f9767615a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1533 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07e50774089f462aa0d2a67a62363acf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/220 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08b36e0a5c8544eaae19bf4314d1e7b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/438 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc5f044695b543d4b19c1b591c0a0eed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb1387521bef4c3fa0db27cf14e27dad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd13af10f02b48a382a1b902e46f25f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1de9b2a30f524c7f86b5bfb2338b385e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09525c0251cf4dd5af94cb879f765681"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1533 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74cd8cde57314860bfd1aaef7d193da2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/220 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c31ca45a30044ae195142b3b4a1a8993"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/438 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee581bbaf74c405c9020e76637fe4796"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f66fe41d64d94cc89453ff1cd4e85435"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  Â·Â·Â·Â·Â·Â·Â·Â·\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112614711110937, max=1.0â€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a483a8968659403b8f27b6ad51fd0e60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241117_231416-x5e1v7i4</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/akramchorfi363-university-hannover/huggingface/runs/x5e1v7i4' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/akramchorfi363-university-hannover/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/akramchorfi363-university-hannover/huggingface' target=\"_blank\">https://wandb.ai/akramchorfi363-university-hannover/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/akramchorfi363-university-hannover/huggingface/runs/x5e1v7i4' target=\"_blank\">https://wandb.ai/akramchorfi363-university-hannover/huggingface/runs/x5e1v7i4</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='288' max='288' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [288/288 04:37, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.435400</td>\n      <td>0.006849</td>\n      <td>0.127099</td>\n      <td>0.209615</td>\n      <td>0.177994</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.396096</td>\n      <td>0.299087</td>\n      <td>0.346344</td>\n      <td>0.559602</td>\n      <td>0.319579</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.386056</td>\n      <td>0.358447</td>\n      <td>0.362802</td>\n      <td>0.611395</td>\n      <td>0.296117</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"('./appropriateness-classifier/tokenizer_config.json',\n './appropriateness-classifier/special_tokens_map.json',\n './appropriateness-classifier/vocab.txt',\n './appropriateness-classifier/added_tokens.json')"},"metadata":{}}],"execution_count":1},{"cell_type":"markdown","source":"**Classify Text based on trained model **","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer, BertForSequenceClassification\n\n# Path to the saved model directory\nMODEL_PATH = './appropriateness-classifier'\n\n# Load the tokenizer and model from the saved directory\nclassifier_tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)\nclassifier_model = BertForSequenceClassification.from_pretrained(MODEL_PATH)\n\ndef classify_text(text):\n    \"\"\"Classify text into inappropriateness categories.\"\"\"\n    # Tokenize input text\n    inputs = classifier_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n\n    # Perform inference\n    with torch.no_grad():\n        outputs = classifier_model(**inputs)\n        logits = outputs.logits  # Extract logits\n\n    # Apply sigmoid to get probabilities for each label\n    probabilities = torch.sigmoid(logits).squeeze().cpu().numpy()\n\n    # Define a threshold for each label (usually 0.5 for binary classification)\n    predictions = (probabilities >= 0.5).astype(int)  # Convert probabilities to 0 or 1\n\n    # Define label names as per your taxonomy\n    label_names = [\n        \"Inappropriateness\", \"Toxic Emotions\", \"Excessive Intensity\", \"Emotional Deception\",\n        \"Missing Commitment\", \"Missing Seriousness\", \"Missing Openness\", \"Missing Intelligibility\",\n        \"Unclear Meaning\", \"Missing Relevance\", \"Confusing Reasoning\", \"Other Reasons\",\n        \"Detrimental Orthography\", \"Reason Unclassified\"\n    ]\n\n    # Combine labels with predictions\n    results = {label: int(prediction) for label, prediction in zip(label_names, predictions)}\n    results_array = [int(prediction) for label, prediction in zip(label_names, predictions)]\n    return results,results_array\n\ntext = \"for everyone who is talking about RAPE in this subject let me ask you one thing!!!! if you got in a huge fight with someone and ended up breaking your hand or arm... would you cut it off just because it would REMIND you of that experience??? if your actually SANE you would say no and if you say yes you need to see a Physiatrist!!!!\"\n\nresult,result_array = classify_text(text)\nprint(result)\nprint(result_array)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T23:02:27.720557Z","iopub.execute_input":"2024-11-17T23:02:27.720954Z","iopub.status.idle":"2024-11-17T23:02:28.020893Z","shell.execute_reply.started":"2024-11-17T23:02:27.720917Z","shell.execute_reply":"2024-11-17T23:02:28.019807Z"}},"outputs":[{"name":"stdout","text":"{'Inappropriateness': 1, 'Toxic Emotions': 0, 'Excessive Intensity': 0, 'Emotional Deception': 0, 'Missing Commitment': 1, 'Missing Seriousness': 0, 'Missing Openness': 0, 'Missing Intelligibility': 1, 'Unclear Meaning': 0, 'Missing Relevance': 0, 'Confusing Reasoning': 0, 'Other Reasons': 0, 'Detrimental Orthography': 0, 'Reason Unclassified': 0}\n[1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import precision_recall_fscore_support\n\nDIMS = [\n    'Inappropriateness',\n    'Toxic Emotions',\n    'Excessive Intensity',\n    'Emotional Deception',\n    'Missing Commitment',\n    'Missing Seriousness',\n    'Missing Openness',\n    'Missing Intelligibility',\n    'Unclear Meaning',\n    'Missing Relevance',\n    'Confusing Reasoning',\n    'Other Reasons',\n    'Detrimental Orthography',\n    'Reason Unclassified'\n]\n\nground_truth = [\n    [1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0],\n    [1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n    [1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n]\nprediction = [\n    [1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0],\n    [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n    [1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0],\n    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n]\n\nfor j, dim in enumerate(DIMS):\n    scores = precision_recall_fscore_support([x[j] for x in ground_truth], [x[j] for x in prediction], average='macro')\n    print('Macro-F1 ' + dim + ': ', scores[2])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T12:25:44.013271Z","iopub.execute_input":"2024-11-16T12:25:44.013818Z","iopub.status.idle":"2024-11-16T12:25:44.05558Z","shell.execute_reply.started":"2024-11-16T12:25:44.013764Z","shell.execute_reply":"2024-11-16T12:25:44.054258Z"}},"outputs":[],"execution_count":null}]}